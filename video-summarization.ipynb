{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Video Summarization Of TV SUM**","metadata":{"_uuid":"fb248b90-7806-4aaa-b53e-cec0fb2b1dfe","_cell_guid":"7cb610f3-083a-4680-93db-53a4bc624ec1","trusted":true}},{"cell_type":"markdown","source":"**Importing necessay libraries**","metadata":{"_uuid":"b54231cd-de98-4c0d-a44b-5cc3b8989729","_cell_guid":"6f721b89-09c8-43fa-b0d4-127f82657784","trusted":true}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport scipy.io as spio\nfrom tqdm import tqdm\nfrom keras.preprocessing import image\nimport keras\nimport math\nimport h5py\nimport json\nimport datetime\nfrom ortools.algorithms import pywrapknapsack_solver","metadata":{"_uuid":"9bebc562-0924-4f82-a8ff-7ed4dae55cc8","_cell_guid":"42c3414f-db6e-4e54-905d-28c191ca1ab5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = ['91IHQYk1IQM', 'EE-bNr36nyA', 'JgHubY5Vw3Y', 'RBCABdttQmI', 'Bhxk-O1Y7Ho', 'XkqCExn6_Us', 'HT5vyqe0Xaw', 'iVt07TCkFM0', 'E11zDS9XGzg', 'qqR6AEXwxoQ', 'cjibtmSLxQ4', '3eYKfiOEJNs', 'AwmHb44_ouw', 'akI8YFjEmUw', 'kLxoNp-UchI', 'GsAD1KT1xo8', 'EYqVtI9YWJA', 'Se3oxnaPsz0', 'vdmoEJ5YbrQ', 'VuWGsYPqAX8', '37rzWOQsNIw', 'oDXZc0tZe04', '4wU_LUjG5Ic', 'J0nA4VgnoCo', 'i3wAGJaaktw', 'sTEELN-vY30', 'xwqBXPGE9pQ', 'PJrm840pAUI', 'gzDbaEs1Rlg', 'uGu_10sucQo', 'WG0MBPpPC6I', '-esJrBWj2d8', 'xxdtq8mxegs', 'WxtbjNsCQ8A', 'fWutDQy1nnY', 'Hl-__g2gn_A', '98MoyGZKHXc', 'byxOvuiIJV0', 'NyBmCxDoHJU', 'b626MiF1ew4', 'XzYM3PfTM4w', 'Yi4Ij2NM7U4', 'xmEERLqJ2kU', 'LRw_obCPUt0', '_xMr-HKMfVA', 'eQu1rNs0an0', '0tmA_C6XwfM', 'jcoYJXDG9sw', 'z_6gVvQb2d0', 'JKpqYvAdIsw']\n","metadata":{"_uuid":"5310cf63-470d-45ac-891d-e91eaeaa3eeb","_cell_guid":"4721a1ca-38ef-4378-ad01-2363ddfcda38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TV SUM FILES ADD\n\nfinalFiles = {}\nfor i in range(len(files)):\n    finalFiles.setdefault(\"video_\"+str(i+1),files[i])\nprint(finalFiles)","metadata":{"_uuid":"64a955cf-9a9a-4205-b851-0072cabec784","_cell_guid":"d6b33b02-63ff-474b-b694-10ae12671383","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(finalFiles)","metadata":{"_uuid":"837ef6db-fcbd-4b8f-a9a5-c83f1d5728eb","_cell_guid":"496ecb18-1445-491e-8b97-91f97e65c8a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No. of frames taken for each batch\nNo_of_frames = 156","metadata":{"_uuid":"0cc54134-1efe-47a9-af70-0770a6e0871a","_cell_guid":"a14ae909-0a5c-4c0b-97db-4c88e1f2ad94","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading Ground Truth**","metadata":{"_uuid":"f6c17a86-3c02-4ee6-8d85-8330da711238","_cell_guid":"c8a16fb8-6198-40ae-bec2-36e56f83c09b","trusted":true}},{"cell_type":"code","source":"def RetrieveGroundTruth(VideoName):\n        \n    GroundTruth = spio.loadmat('/kaggle/input/tv-sum-gt/'+VideoName+'.mat', squeeze_me=True)\n    \n    print(len(GroundTruth['gt_score']))\n        \n    Y = np.zeros( shape=(20000,)  , dtype=np.float16)\n\n    for i in range(0, len(GroundTruth['gt_score']) ):\n        Y[i]=GroundTruth['gt_score'][i]\n\n    return Y,len(GroundTruth['gt_score'])","metadata":{"_uuid":"9bd2d38a-9120-4454-90ad-0084597ecbe5","_cell_guid":"37757c4e-2e19-4b4d-8dec-1d6e45d57aab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generator-3**","metadata":{"_uuid":"c17da4c6-609c-4dca-aa03-a1e1007728e2","_cell_guid":"09447e9c-4050-4f7d-9f3b-074615f447a9","trusted":true}},{"cell_type":"code","source":"def GeneratorForVideos(VideosName):\n    \n    np.random.shuffle(VideosName)\n\n    for VideoNo in range(len(VideosName)):\n        \n        VideoName = finalFiles[VideosName[VideoNo].decode('utf-8')]\n     \n        Y,length = RetrieveGroundTruth(VideoName)\n        \n        temp = np.load('/kaggle/input/tv-sum/Npy/'+VideoName+'.npy')\n\n        batches = math.ceil(length/No_of_frames)\n        train_image = np.zeros((No_of_frames,156,156,3),dtype=np.float16)\n        j = 0\n        for i in range(0,batches):\n            if i != (batches - 1):\n                train_image = temp[j:j+No_of_frames]\n                GY  = Y[j:j+No_of_frames]\n                X = train_image/255\n                train_image = np.zeros((No_of_frames,156,156,3),dtype=np.float16)\n                X = np.expand_dims(X,axis = 0)\n                GY = np.expand_dims(GY, axis = 0)\n                j = j + No_of_frames\n                yield X,GY\n            else:\n                train_image[0:(length - (i * No_of_frames) )] = temp[j:j+(length - (i * No_of_frames) )]\n                GY  = Y[j:j+No_of_frames]\n                X = train_image/255\n                train_image = np.zeros((No_of_frames,156,156,3),dtype=np.float16)\n                X = np.expand_dims(X,axis = 0)\n                GY = np.expand_dims(GY, axis = 0)\n                j = j + No_of_frames\n                yield X,GY","metadata":{"_uuid":"e6f6c4ba-7274-43c9-94f9-7f260403522f","_cell_guid":"01b77a8f-d78f-4c3a-9b33-f4efc22f43e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open('/kaggle/input/splits/tvsum_splits.json')\nsplits = json.load(f)","metadata":{"_uuid":"4f24c724-e840-4d85-87a4-2c5a07e779cb","_cell_guid":"52bf77dd-bf38-4fc0-a50f-094581b3d7ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_keys = splits[4]['train_keys']\ntest_keys = splits[4]['test_keys']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"partition = {\n    'train': train_keys,\n    'validation':test_keys\n}","metadata":{"_uuid":"a71dd299-a81c-403c-9379-4580e8085fa2","_cell_guid":"b1e7032b-d2f0-4136-8894-41398f5219b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_generator= tf.data.Dataset.from_generator(GeneratorForVideos , args=[partition['train']] ,output_types=(tf.float16, tf.float16) ,output_shapes=((1,No_of_frames, 156, 156, 3), (1,No_of_frames)  )   )\nvalidation_generator=  tf.data.Dataset.from_generator(GeneratorForVideos , args=[partition['validation']] ,output_types=(tf.float16, tf.float16) ,output_shapes=((1,No_of_frames, 156, 156, 3), (1,No_of_frames)  )   )","metadata":{"_uuid":"c79991f2-d44e-4229-ba08-cb594fda8727","_cell_guid":"b720860c-d145-4ecf-8aff-a8d999fe1be2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining the Model**","metadata":{"_uuid":"22f41f89-a2d6-429d-8513-fd090376edcd","_cell_guid":"2d4c606b-a773-4ab1-a706-cce56f63896e","trusted":true}},{"cell_type":"code","source":"class StepOneAttention(tf.keras.Model):\n    def __init__(self, units):\n        super(StepOneAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, features, hidden):\n        # features(CNN_encoder output) shape == (batch_size, no. of frames, height, width, channels)\n\n        # hidden shape == (batch_size, hidden_size)\n        # hidden_with_time_axis shape == (batch_size, 1,1,1, hidden_size)\n#         print(\"Attention Step 1\")\n        hidden = tf.expand_dims(hidden, 1)\n        hidden = tf.expand_dims(hidden, 1)\n        hidden = tf.expand_dims(hidden, 1)\n  \n        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden)))\n        \n        # This gives you an unnormalized score for each image feature.\n        score = self.V(attention_hidden_layer)\n        \n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # context_vector shape after sum == (batch_size,no. of frames, height, width, channels)\n        context_vector = attention_weights * features\n\n        return context_vector,attention_weights","metadata":{"_uuid":"0670f198-e623-4432-be0c-600912ffc667","_cell_guid":"cfdb3c0f-442d-48d3-a866-594fcb1d6b05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StepTwoAttention(tf.keras.Model):\n    def __init__(self, units):\n        super(StepTwoAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, features, hidden):\n\n        # hidden shape == (batch_size, hidden_size)\n        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n#         print(\"Attention Step 2\")\n        features = tf.reduce_sum(features, axis=2)\n        features = tf.reduce_sum(features, axis=2)\n        # features(CNN_encoder output) shape == (batch_size, no. of frames, channels)\n        hidden = tf.expand_dims(hidden, 1)\n        \n        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                             self.W2(hidden)))\n\n        # This gives you an unnormalized score for each image feature.\n        score = self.V(attention_hidden_layer)\n        \n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # context_vector shape after sum == (batch_size,  no. of frames, channels)\n        context_vector = attention_weights * features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector,attention_weights","metadata":{"_uuid":"8a01e180-1f3b-48aa-860f-798d5cfc0cbb","_cell_guid":"5d27f0d5-3dee-4e76-874e-5e1da783b897","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self,units):\n        super(Decoder, self).__init__()\n        self.units = units\n\n        self.gru = tf.keras.layers.GRU(self.units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.fc1 = tf.keras.layers.Dense(300,activation='relu')\n        self.fc2 = tf.keras.layers.Dense(1,activation='relu')\n        self.step_one_attention = StepOneAttention(self.units)\n        self.step_two_attention = StepTwoAttention(self.units)\n\n    def call(self,features, hidden):\n        # defining attention as a separate model\n        first_context_vector, attention_weights = self.step_one_attention(features, hidden)\n        context_vector, attention_weights = self.step_two_attention(first_context_vector, hidden)\n        hidden = tf.expand_dims(hidden, 1)\n        \n        # x shape after concatenation == (batch_size, 1, context_vector_size + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), hidden], axis=-1)\n        \n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n\n        # shape == (batch_size, max_length, hidden_size)\n        x = self.fc1(output)\n        \n        x = tf.reshape(x, (-1, x.shape[2]))\n\n        # output shape == (batch_size * max_length, frame score)\n        x = self.fc2(x)\n        return x, state, attention_weights","metadata":{"_uuid":"9696d4d6-a673-4497-b7d2-1b4b628d9720","_cell_guid":"1c3b095e-e8fe-4081-b1b7-e5f93dd3cdf4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = Decoder(1024)","metadata":{"_uuid":"248a0748-81f6-4efc-a78c-13ba50fdd72a","_cell_guid":"1711cef7-8ecb-441d-a29f-0cbfa102e992","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def multiAttention(features):\n    outputs = tf.zeros((1,1))\n    hidden = tf.zeros((1,1024)) # decoder hidden state \n    for i in range(No_of_frames):\n        prediction, hidden ,_= decoder(features,hidden)\n        if i == 0:\n            outputs = prediction\n        else:\n            outputs = tf.concat((outputs,prediction),axis =-1)\n    return outputs","metadata":{"_uuid":"989410f0-944d-487c-9614-7e9a44292e64","_cell_guid":"768638ee-d908-4c20-a6ba-372493dfe568","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv3DMultiAttentionModel():\n\n    X_input  = tf.keras.Input(shape=(No_of_frames,156,156,3))\n\n    X = tf.keras.layers.Conv3D(32,(3,3,3),activation='relu',padding='same')(X_input)\n    X = tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(X)\n\n    X = tf.keras.layers.Conv3D(64,(3,3,3),activation='relu',padding='same')(X)\n    X = tf.keras.layers.MaxPooling3D((1,2,2))(X)\n    X = tf.keras.layers.BatchNormalization()(X)\n\n    X = tf.keras.layers.Conv3D(128,(3,3,3),activation='relu',padding='same')(X)\n    X = tf.keras.layers.MaxPooling3D((1,2,2))(X)\n    X = tf.keras.layers.BatchNormalization()(X)\n\n\n    X = tf.keras.layers.Conv3D(256,(1,1,1),activation='relu',padding='same')(X)\n    X = tf.keras.layers.MaxPooling3D((1,2,2))(X)\n    X = tf.keras.layers.BatchNormalization()(X)\n\n    X = tf.keras.layers.Conv3D(1024,(1,1,1),activation='relu',padding='same')(X)\n    X = tf.keras.layers.MaxPooling3D((1,2,2))(X)\n    \n    # Decoder with Multi-Attention\n    X = multiAttention(X)\n\n    output =tf.keras.layers.Dense(No_of_frames,activation='sigmoid')(X)\n\n\n    model = tf.keras.Model(inputs=X_input,outputs=output)\n\n    return model","metadata":{"_uuid":"bcd3ff4b-066e-4bca-9532-1b414ab52fd6","_cell_guid":"4bf38a14-308f-4b5e-8b21-0975fa1399a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = conv3DMultiAttentionModel()","metadata":{"_uuid":"b3542fdd-32af-479b-82be-0bbde05c0c71","_cell_guid":"a835eed7-f12c-4d45-8cd7-739d173389d1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"/kaggle/input/tvsummodelsplit1/model.epoch08-loss0.014.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=1e-4)","metadata":{"_uuid":"2d16b843-ef90-4fec-ad7b-f38533a6ef3f","_cell_guid":"e2086561-aaca-40ae-9a98-958a0a3b5bfc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='mean_squared_error',optimizer=opt,metrics=['mae'])","metadata":{"_uuid":"547e7e04-5a0a-41cb-9c68-99aef55bcb8a","_cell_guid":"b2f492ee-112d-43f0-accf-0665aeb27e95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_dir = \"../working/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)","metadata":{"_uuid":"4489b46a-881f-4a6c-8876-6972f8ba20d6","_cell_guid":"3c7cc910-b427-4975-9767-bb14468e0e2a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('model.epoch={epoch:02d}-loss={loss:.3f}.h5', monitor='val_accuracy', verbose=1, save_weights_only=True,save_best_only=False, mode='max')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    training_generator,\n    epochs = 8,\n    validation_data=validation_generator,\n    verbose=2,\n    callbacks=[checkpoint,tensorboard_callback]\n    )","metadata":{"_uuid":"0347c755-b2d5-4574-9563-d3ddbc598402","_cell_guid":"31d1b4e6-1e19-4ab9-b6dc-156848b001f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mae = history.history['mae']\nmse = history.history['loss']\nval_mae = history.history['val_mae']\nval_mse = history.history['val_loss']\n\nepochs = range(len(mae))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.plot(epochs,mae,label='train')\nplt.plot(epochs,val_mae,label='valid')\nplt.title('Training and validation MAE')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.plot(epochs,mse,label='train')\nplt.plot(epochs,val_mse,label='valid')\nplt.title('Training and validation MSE / loss')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}